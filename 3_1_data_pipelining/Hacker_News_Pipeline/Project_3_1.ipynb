{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this guided project, we will use the pipeline we have been building, and apply it to a real world data pipeline project. From a JSON API, we will filter, clean, aggregate, and summarize data in a sequence of tasks that will apply these transformations for us.\n",
    "\n",
    "The data we will use comes from a [Hacker News](https://news.ycombinator.com/) (HN) API that returns JSON data of the top stories in 2014.\n",
    "\n",
    "To make things easier, we have already downloaded a list of JSON posts to a file called _hn_stories_2014.json_. The JSON file contains a single key **stories**, which contains a list of stories (posts). Each post has a set of keys, but we will deal only with the following keys:\n",
    "- **created_at**: A timestamp of the story's creation time.\n",
    "- **created_at_i**: A unix epoch timestamp.\n",
    "- **url**: The URL of the story link.\n",
    "- **objectID**: The ID of the story.\n",
    "- **author**: The story's author (username on HN).\n",
    "- **points**: The number of upvotes the story had.\n",
    "- **title**: The headline of the post.\n",
    "- **num_comments**: The number of a comments a post has.\n",
    "\n",
    "Using this dataset, we will run a sequence of basic natural language processing tasks using **our Pipeline class**. The goal will be to find the top 100 keywords of Hacker News posts in 2014. Because Hacker News is the most popular technology social media site, this will give us an understanding of the most talked about tech topics in 2014!\n",
    "\n",
    "## Importing files and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can import Pipeline class from the external python pipeline.py file\n",
    "from pipeline import Pipeline, build_csv\n",
    "from stop_words import stop_words\n",
    "\n",
    "import json\n",
    "import io\n",
    "import datetime as dt\n",
    "import csv\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data\n",
    "\n",
    "We'll start the project by loading the JSON file data into Python. Because JSON files resemble a key-value dictionary, the goal is to parse the JSON file into a Python dict object. We can accomplish this using the **json module**.\n",
    "\n",
    "To load in a file, json exposes a method called json.load() which takes in a Python file object as the first argument. Using this json.load() method, we'll load the hn_stories_2014.json file as a Python dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open(\"my_datasets/hn_stories_2014.json\",\"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data[\"stories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stories\n",
    "\n",
    "Let's start by filtering the list of stories to get the most popular stories of the year.\n",
    "\n",
    "Like any social link aggregator site, individual users can post whatever content they want. The reason we want the most popular stories is to ensure that we select stories that were the most talked about during the year. We can filter for popular stories by ensuring they are links (not Ask HN posts), have a good number of points, and have some comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_interesting(story):\n",
    "        interesting = False\n",
    "        if story[\"points\"] > 50 and story[\"num_comments\"] > 1 and not story[\"title\"].startswith(\"Ask HN\"):\n",
    "            interesting = True\n",
    "        return interesting\n",
    "    return (story for story in stories if is_interesting(story))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "\n",
    "With a reduced set of stories, it's time to write these dict objects to a CSV file. The purpose of translating the dictionaries to a CSV is that we want to have a consistent data format when running the later summarizations. By keeping consistent data formats, each of your pipeline tasks will be adaptable with future task requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    header = ['objectID', 'created_at', 'url', 'points', 'title']\n",
    "    \n",
    "    def get_fields(stories, header):\n",
    "        for story in stories:\n",
    "            new_story = []\n",
    "            for field in header:\n",
    "                if field != \"created_at\":\n",
    "                    new_story.append(story[field])\n",
    "                else:\n",
    "                    date = dt.datetime.strptime(story[field],\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                    new_story.append(date)\n",
    "            yield new_story\n",
    "            \n",
    "    result = build_csv(get_fields(stories, header), header=header, file=io.StringIO())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column\n",
    "\n",
    "Using the CSV file format we created in the previous task, we can now extract the title column. Once we have extracted the titles of each popular post, we can then run the next word frequency task.\n",
    "\n",
    "The steps are:\n",
    "1. Import csv, and create a csv.reader() object from the file object. \n",
    "2. Find the index of the title in the header.\n",
    "3. Iterate the through the reader, and return each item from the reader in the corresponding title index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    read = csv.reader(csv_file, delimiter=\",\")\n",
    "    header = next(read)\n",
    "    idx = header.index(\"title\")\n",
    "    return (row[idx] for row in read)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles\n",
    "\n",
    "Because we're trying to create a word frequency model of words from Hacker News titles, we need a way to create a consistent set of words to use. For example, words like _Google_, _google_, _GooGle?_, and _google._, all mean the same keyword: **google**. If we were to split the title into words, however, they would all be lumped into different categories.\n",
    "\n",
    "To clean the titles, we should make sure to lower case the titles, and to remove the punctuation. An easy way to rid a string of punctuation is to check each character, determine if it is a letter or punctuation, and only keep the letter.\n",
    "\n",
    "From the **string** package, we are given a handy string constant that contains all the punctuation needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_titles(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(c for c in title if c not in string.punctuation)\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Word Frequency Dictionary\n",
    "\n",
    "With a cleaned title, we can now build the word frequency dictionary. However, to find actual keywords, we should enforce the word frequency dictionary to not include **stop words**. Stop words are words that occur frequently in language like \"the\", \"or\", etc., and are commonly rejected in keyword searches.\n",
    "\n",
    "We have included a module called stop_words with a tuple of the most common used stop words in the English language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=clean_titles)\n",
    "def build_keyword_dictionary(titles):\n",
    "    kw_dictionary = {}\n",
    "    for title in titles:\n",
    "        word_list = title.split(\" \")\n",
    "        for word in word_list:\n",
    "            if word not in stop_words and word:\n",
    "                if word not in kw_dictionary:\n",
    "                    kw_dictionary[word] = 0\n",
    "                kw_dictionary[word] += 1\n",
    "    return kw_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Top Words\n",
    "\n",
    "Finally, we're ready to sort the top words used in all the titles. The goal is to output the **Top 100** list of tuples with (word, frequency) as the entries sorted from most used, to least most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def sort_words(kw_dict):\n",
    "    top_words = [(k,v) for k,v in kw_dict.items()]\n",
    "    top_words.sort(reverse=True, key=lambda x: x[1])\n",
    "    return top_words[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the pipeline\n",
    "\n",
    "Let's run the pipline using pipeline.run() and check the Top 100 list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('new', 185),\n",
       " ('google', 167),\n",
       " ('bitcoin', 101),\n",
       " ('open', 92),\n",
       " ('programming', 90),\n",
       " ('web', 88),\n",
       " ('data', 85),\n",
       " ('video', 79),\n",
       " ('python', 75),\n",
       " ('code', 72),\n",
       " ('facebook', 71),\n",
       " ('released', 71),\n",
       " ('using', 70),\n",
       " ('2013', 65),\n",
       " ('javascript', 65),\n",
       " ('free', 64),\n",
       " ('source', 64),\n",
       " ('game', 63),\n",
       " ('internet', 62),\n",
       " ('microsoft', 59),\n",
       " ('c', 59),\n",
       " ('linux', 58),\n",
       " ('app', 57),\n",
       " ('pdf', 55),\n",
       " ('work', 54),\n",
       " ('language', 54),\n",
       " ('software', 52),\n",
       " ('2014', 52),\n",
       " ('startup', 51),\n",
       " ('apple', 50),\n",
       " ('use', 50),\n",
       " ('make', 50),\n",
       " ('time', 48),\n",
       " ('yc', 48),\n",
       " ('security', 48),\n",
       " ('nsa', 45),\n",
       " ('github', 45),\n",
       " ('windows', 44),\n",
       " ('world', 41),\n",
       " ('way', 41),\n",
       " ('like', 41),\n",
       " ('1', 40),\n",
       " ('project', 40),\n",
       " ('computer', 40),\n",
       " ('heartbleed', 40),\n",
       " ('git', 37),\n",
       " ('users', 37),\n",
       " ('dont', 37),\n",
       " ('design', 37),\n",
       " ('ios', 37),\n",
       " ('developer', 36),\n",
       " ('os', 36),\n",
       " ('twitter', 36),\n",
       " ('ceo', 36),\n",
       " ('vs', 36),\n",
       " ('life', 36),\n",
       " ('big', 35),\n",
       " ('day', 35),\n",
       " ('android', 34),\n",
       " ('online', 34),\n",
       " ('years', 33),\n",
       " ('simple', 33),\n",
       " ('court', 33),\n",
       " ('guide', 32),\n",
       " ('learning', 32),\n",
       " ('mt', 32),\n",
       " ('api', 32),\n",
       " ('says', 32),\n",
       " ('apps', 32),\n",
       " ('browser', 32),\n",
       " ('server', 31),\n",
       " ('firefox', 31),\n",
       " ('fast', 31),\n",
       " ('gox', 31),\n",
       " ('problem', 31),\n",
       " ('mozilla', 31),\n",
       " ('engine', 31),\n",
       " ('site', 31),\n",
       " ('introducing', 30),\n",
       " ('amazon', 30),\n",
       " ('year', 30),\n",
       " ('support', 29),\n",
       " ('stop', 29),\n",
       " ('built', 29),\n",
       " ('better', 29),\n",
       " ('million', 29),\n",
       " ('people', 29),\n",
       " ('text', 29),\n",
       " ('3', 28),\n",
       " ('does', 28),\n",
       " ('tech', 28),\n",
       " ('development', 28),\n",
       " ('billion', 27),\n",
       " ('developers', 27),\n",
       " ('just', 27),\n",
       " ('library', 27),\n",
       " ('did', 27),\n",
       " ('website', 27),\n",
       " ('money', 27),\n",
       " ('inside', 27)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pipeline.run()\n",
    "results[sort_words]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
